{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkumXK25_A6S"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmQ102pjyuXF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmdDiORM29Gl"
      },
      "outputs": [],
      "source": [
        "from mscnet import *\n",
        "from loss import *\n",
        "from dataloaders import *\n",
        "from metrics import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWsw2QBXCzld",
        "outputId": "3fb92356-90b9-4f93-fdd8-01f205f26f38"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5-N40ik_W0r"
      },
      "source": [
        "## Network training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T0HrOqTLYfJ"
      },
      "source": [
        "### STARE custom loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q02CCfMbghvQ",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "n_epoch = 100\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "net_custom_loss_stare = MSCNet(n_channels = 1, n_classes = 1)\n",
        "net_custom_loss_stare.to(device)\n",
        "\n",
        "dataset = DataLoaderSTARE(\"data/stare\")\n",
        "train, test = torch.utils.data.random_split(dataset, [15,5], generator = torch.Generator().manual_seed(123))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=1, num_workers=0, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1, num_workers=0, pin_memory=True, drop_last=True)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(net_custom_loss_stare.parameters(), lr = 1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.9)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  running_loss = 0.0\n",
        "  net_custom_loss_stare.train()\n",
        "\n",
        "  for (image, mask_seg1, mask_seg2, mask_center) in tqdm(train_loader):\n",
        "    image = image.to(device)\n",
        "    mask_seg1 = mask_seg1.to(device)\n",
        "    mask_seg2 = mask_seg2.to(device)\n",
        "    mask_center = mask_center.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output_top, output_bot = net_custom_loss_stare(image)\n",
        "\n",
        "\n",
        "    criterion = CustomLoss(0.2, 0.5, 0.1, threshold=0.63)\n",
        "    loss = criterion(torch.squeeze(output_top), torch.squeeze(mask_seg1), torch.squeeze(output_bot), torch.squeeze(mask_center))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  train_loss=running_loss/len(train_loader)\n",
        "  train_losses.append(train_loss)\n",
        "  \n",
        "def test(epoch):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  net_custom_loss_stare.eval()  \n",
        "\n",
        "  with torch.no_grad():\n",
        "    for (image, mask_seg1, mask_seg2, mask_center) in tqdm(test_loader):\n",
        "      image = image.to(device)\n",
        "      mask_seg1 = mask_seg1.to(device)\n",
        "      mask_seg2 = mask_seg2.to(device)\n",
        "      mask_center = mask_center.to(device)\n",
        "      \n",
        "      output_top, output_bot = net_custom_loss_stare(image)\n",
        "\n",
        "      criterion = CustomLoss(0.2, 0.5, 0.1, threshold=0.63)\n",
        "      loss = criterion(torch.squeeze(output_top), torch.squeeze(mask_seg1), torch.squeeze(output_bot), torch.squeeze(mask_center))\n",
        "\n",
        "      running_loss+=loss.item()\n",
        "      \n",
        "  \n",
        "  test_loss=running_loss/len(test_loader)\n",
        "\n",
        "  eval_losses.append(test_loss)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(n_epoch+1):\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  scheduler.step(eval_losses[-1])\n",
        "  print(f\"Epoch {epoch+1}, training_loss = {train_losses[-1]:.6f}, val_loss = {eval_losses[-1]:.6f}\")\n",
        "  if epoch % 50 == 0:\n",
        "    torch.save(net_custom_loss_stare.state_dict(), f'stare_custom_loss_stare_{epoch}.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D6mAP0nAR3-"
      },
      "source": [
        "### Results preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7AuxM3yrB5kf",
        "outputId": "e57da985-c6ed-4f88-f5fe-55dce2af0da4"
      },
      "outputs": [],
      "source": [
        "ses = []\n",
        "sps = []\n",
        "accs = []\n",
        "aucs = []\n",
        "\n",
        "net_custom_loss_stare = MSCNet(n_channels = 1, n_classes = 1)\n",
        "net_custom_loss_stare.to(device)\n",
        "\n",
        "dataset = DataLoaderSTARE(\"data/stare\")\n",
        "train, test = torch.utils.data.random_split(dataset, [15,5], generator = torch.Generator().manual_seed(123))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=1, num_workers=0, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1, num_workers=0, pin_memory=True, drop_last=True)\n",
        "\n",
        "net_custom_loss_stare.load_state_dict(torch.load(\"drive/MyDrive/wb/stare_custom_loss_stare_100.pth\"))\n",
        "\n",
        "\n",
        "for (image, mask_seg1, mask_seg2, mask_center) in test_loader:\n",
        "  image = image.to(device)\n",
        "  output_top, output_bot = net_custom_loss_stare(image)\n",
        "\n",
        "\n",
        "  mask_top = torch.squeeze(output_top.cpu().detach()).numpy()\n",
        "  mask_bot = torch.squeeze(output_bot.cpu().detach()).numpy()\n",
        "\n",
        "  mask_seg1 = torch.squeeze(mask_seg1.cpu().detach()).numpy()\n",
        "  mask_center = torch.squeeze(mask_center.cpu().detach()).numpy()\n",
        "\n",
        "  print(\"--------------------------------------------\")\n",
        "\n",
        "  plt.hist(mask_top.flatten())\n",
        "  plt.title(\"Histogram of segmentation mask\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.hist(mask_top.flatten())\n",
        "  plt.title(\"Histogram of centerline extraction mask\")\n",
        "  plt.show()    \n",
        "\n",
        "\n",
        "  ses.append(SE(mask_top, mask_seg1))\n",
        "  sps.append(SP(mask_top, mask_seg1))\n",
        "  accs.append(accuracy(mask_top, mask_seg1))\n",
        "  aucs.append(AUC(mask_top, mask_seg1))\n",
        "\n",
        "  print(\"Metrics for segmentation: \")\n",
        "  print(\"SE = \",ses[-1])\n",
        "  print(\"SP = \",sps[-1])\n",
        "  print(\"accuracy = \",accs[-1])\n",
        "  print(\"AUC = \", aucs[-1])\n",
        "\n",
        "\n",
        "  print(\"Metrics for centerline extraction: \")\n",
        "  print(\"SE = \",SE(mask_bot, mask_center))\n",
        "  print(\"SP = \",SP(mask_bot, mask_center))\n",
        "  print(\"accuracy = \",accuracy(mask_bot, mask_center))\n",
        "  print(\"AUC = \", AUC(mask_bot, mask_center))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(16,9))\n",
        "  plt.imshow(torch.squeeze(output_top.cpu().detach()).numpy(), cmap=\"gray\")\n",
        "  plt.title(\"Segmentation mask (logits)\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(16,9))\n",
        "  plt.imshow(torch.squeeze(output_bot.cpu().detach()).numpy(), cmap=\"gray\")\n",
        "  plt.title(\"Centerline extraction mask (logits)\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "print(\"DRIVE, custom loss SE = {:.4f}\".format(sum(ses)/len(ses)))\n",
        "print(\"DRIVE, custom loss SP = {:.4f}\".format(sum(sps)/len(sps)))\n",
        "print(\"DRIVE, custom loss ACC = {:.4f}\".format(sum(accs)/len(accs)))\n",
        "print(\"DRIVE, custom loss AUC = {:.4f}\".format(sum(aucs)/len(aucs)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldj9d3P7LBV2"
      },
      "source": [
        "### STARE BCE loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM-KzEjAxVUO"
      },
      "outputs": [],
      "source": [
        "n_epoch = 100\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "net_BCE_loss_stare = MSCNet(n_channels = 1, n_classes = 1)\n",
        "net_BCE_loss_stare.to(device)\n",
        "\n",
        "dataset = DataLoaderSTARE(\"data/stare\")\n",
        "train, test = torch.utils.data.random_split(dataset, [15,5], generator = torch.Generator().manual_seed(123))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=1, num_workers=0, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1, num_workers=0, pin_memory=True, drop_last=True)\n",
        "\n",
        "optimizer = optim.Adam(net_BCE_loss_stare.parameters(), lr = 1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.9)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  running_loss = 0.0\n",
        "  net_BCE_loss_stare.train()\n",
        "\n",
        "  for (image, mask_seg1, mask_seg2, mask_center) in tqdm(train_loader):\n",
        "    image = image.to(device)\n",
        "    mask_seg1 = mask_seg1.to(device)\n",
        "    mask_seg2 = mask_seg2.to(device)\n",
        "    mask_center = mask_center.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output_top, output_bot = net_BCE_loss_stare(image)\n",
        "\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    loss = criterion(torch.squeeze(output_top), torch.squeeze(mask_seg1)) + criterion(torch.squeeze(output_bot), torch.squeeze(mask_seg2))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  train_loss=running_loss/len(train_loader)\n",
        "  train_losses.append(train_loss)\n",
        "  \n",
        "def test(epoch):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  net_BCE_loss_stare.eval()  \n",
        "\n",
        "  with torch.no_grad():\n",
        "    for (image, mask_seg1, mask_seg2, mask_center) in tqdm(test_loader):\n",
        "      image = image.to(device)\n",
        "      mask_seg1 = mask_seg1.to(device)\n",
        "      mask_seg2 = mask_seg2.to(device)\n",
        "      mask_center = mask_center.to(device)\n",
        "      \n",
        "      output_top, output_bot = net_BCE_loss_stare(image)\n",
        "\n",
        "      criterion = nn.BCEWithLogitsLoss()\n",
        "      loss = criterion(torch.squeeze(output_top), torch.squeeze(mask_seg1)) + criterion(torch.squeeze(output_bot), torch.squeeze(mask_seg2)) \n",
        "\n",
        "      running_loss+=loss.item()\n",
        "      \n",
        "  \n",
        "  test_loss=running_loss/len(test_loader)\n",
        "\n",
        "  eval_losses.append(test_loss)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(n_epoch+1):\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  scheduler.step(eval_losses[-1])\n",
        "  print(f\"Epoch {epoch+1}, training_loss = {train_losses[-1]:.6f}, val_loss = {eval_losses[-1]:.6f}\")\n",
        "  if epoch % 50 == 0:\n",
        "    torch.save(net_BCE_loss_stare.state_dict(), f'stare_BCE_loss_stare_{epoch}.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aENKbeL3Lrkz"
      },
      "source": [
        "### Results preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qfdHUnrDGIXd",
        "outputId": "c3acd43f-657e-4493-fe0f-f5671e1fc948"
      },
      "outputs": [],
      "source": [
        "ses = []\n",
        "sps = []\n",
        "accs = []\n",
        "aucs = []\n",
        "\n",
        "net_BCE_loss_stare = MSCNet(n_channels = 1, n_classes = 1)\n",
        "net_BCE_loss_stare.to(device)\n",
        "\n",
        "dataset = DataLoaderSTARE(\"data/stare\")\n",
        "train, test = torch.utils.data.random_split(dataset, [15,5], generator = torch.Generator().manual_seed(123))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=1, num_workers=0, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1, num_workers=0, pin_memory=True, drop_last=True)\n",
        "\n",
        "net_BCE_loss_stare.load_state_dict(torch.load(\"drive/MyDrive/wb/stare_BCE_loss_stare_100.pth\"))\n",
        "\n",
        "for (image, mask_seg1, mask_seg2, mask_center) in test_loader:\n",
        "  image = image.to(device)\n",
        "  output_top, output_bot = net_BCE_loss_stare(image)\n",
        "\n",
        "\n",
        "  mask_top = torch.squeeze(output_top.cpu().detach()).numpy()\n",
        "  mask_bot = torch.squeeze(output_bot.cpu().detach()).numpy()\n",
        "\n",
        "  mask_seg1 = torch.squeeze(mask_seg1.cpu().detach()).numpy()\n",
        "  mask_seg2 = torch.squeeze(mask_seg2.cpu().detach()).numpy()\n",
        "\n",
        "  print(\"--------------------------------------------\")\n",
        "\n",
        "  plt.hist(mask_top.flatten())\n",
        "  plt.title(\"Histogram of segmentation mask\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.hist(mask_top.flatten())\n",
        "  plt.title(\"Histogram of negative segmentation mask\")\n",
        "  plt.show()    \n",
        "\n",
        "\n",
        "  ses.append(SE(mask_top, mask_seg1))\n",
        "  sps.append(SP(mask_top, mask_seg1))\n",
        "  accs.append(accuracy(mask_top, mask_seg1))\n",
        "  aucs.append(AUC(mask_top, mask_seg1))\n",
        "\n",
        "  print(\"Metrics for segmentation: \")\n",
        "  print(\"SE = \",ses[-1])\n",
        "  print(\"SP = \",sps[-1])\n",
        "  print(\"accuracy = \",accs[-1])\n",
        "  print(\"AUC = \", aucs[-1])\n",
        "\n",
        "\n",
        "\n",
        "  print(\"Metrics for negative segmentation: \")\n",
        "  print(\"SE = \",SE(mask_bot, mask_seg2))\n",
        "  print(\"SP = \",SP(mask_bot, mask_seg2))\n",
        "  print(\"accuracy = \",accuracy(mask_bot, mask_seg2))\n",
        "  print(\"AUC = \", AUC(mask_bot, mask_seg2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(16,9))\n",
        "  plt.imshow(torch.squeeze(output_top.cpu().detach()).numpy(), cmap=\"gray\")\n",
        "  plt.title(\"Segmentation mask (logits)\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(16,9))\n",
        "  plt.imshow(torch.squeeze(output_bot.cpu().detach()).numpy(), cmap=\"gray\")\n",
        "  plt.title(\"Negative segmentation mask (logits)\")\n",
        "  plt.show()\n",
        "\n",
        "print(\"DRIVE, custom loss SE = {:.4f}\".format(sum(ses)/len(ses)))\n",
        "print(\"DRIVE, custom loss SP = {:.4f}\".format(sum(sps)/len(sps)))\n",
        "print(\"DRIVE, custom loss ACC = {:.4f}\".format(sum(accs)/len(accs)))\n",
        "print(\"DRIVE, custom loss AUC = {:.4f}\".format(sum(aucs)/len(aucs)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M_ZO01FLbNV"
      },
      "source": [
        "### CHASE custom loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "yb9v-NAO_vbY",
        "outputId": "b3e29a2f-9ace-4113-b564-a3e5771f7971"
      },
      "outputs": [],
      "source": [
        "n_epoch = 100\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "net_custom_loss_chase = MSCNet(n_channels = 1, n_classes = 1, inner_sizes=(59, 114, 234, 474), output_size=960)\n",
        "net_custom_loss_chase.to(device)\n",
        "\n",
        "dataset = DataLoaderCHASE(\"data/chase\")\n",
        "train, test = torch.utils.data.random_split(dataset, [21,7], generator = torch.Generator().manual_seed(123))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=1, num_workers=0, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1, num_workers=0, pin_memory=True, drop_last=True)\n",
        "\n",
        "optimizer = optim.Adam(net_custom_loss_chase.parameters(), lr = 1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.9)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  running_loss = 0.0\n",
        "  net_custom_loss_chase.train()\n",
        "\n",
        "  for (image, mask_seg1, mask_seg2, mask_center) in tqdm(train_loader):\n",
        "    image = image.to(device)\n",
        "    mask_seg1 = mask_seg1.to(device)\n",
        "    mask_seg2 = mask_seg2.to(device)\n",
        "    mask_center = mask_center.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output_top, output_bot = net_custom_loss_chase(image)\n",
        "\n",
        "\n",
        "    criterion = CustomLoss(0.2, 0.5, 0.1, threshold=0.63)\n",
        "    loss = criterion(torch.squeeze(output_top), torch.squeeze(mask_seg1), torch.squeeze(output_bot), torch.squeeze(mask_center))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  train_loss=running_loss/len(train_loader)\n",
        "  train_losses.append(train_loss)\n",
        "  \n",
        "def test(epoch):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  net_custom_loss_chase.eval()  \n",
        "\n",
        "  with torch.no_grad():\n",
        "    for (image, mask_seg1, mask_seg2, mask_center) in tqdm(test_loader):\n",
        "      image = image.to(device)\n",
        "      mask_seg1 = mask_seg1.to(device)\n",
        "      mask_seg2 = mask_seg2.to(device)\n",
        "      mask_center = mask_center.to(device)\n",
        "      \n",
        "      output_top, output_bot = net_custom_loss_chase(image)\n",
        "\n",
        "      criterion = CustomLoss(0.2, 0.5, 0.1, threshold=0.63)\n",
        "      loss = criterion(torch.squeeze(output_top), torch.squeeze(mask_seg1), torch.squeeze(output_bot), torch.squeeze(mask_center))\n",
        "\n",
        "      running_loss+=loss.item()\n",
        "      \n",
        "  \n",
        "  test_loss=running_loss/len(test_loader)\n",
        "\n",
        "  eval_losses.append(test_loss)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(n_epoch+1):\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  scheduler.step(eval_losses[-1])\n",
        "  print(f\"Epoch {epoch+1}, training_loss = {train_losses[-1]:.6f}, val_loss = {eval_losses[-1]:.6f}\")\n",
        "  if epoch % 50 == 0:\n",
        "    torch.save(net_custom_loss_chase.state_dict(), f'custom_loss_chase_{epoch}.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGRYCM_ALifl"
      },
      "source": [
        "### Results preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBpgdii9B_Tw"
      },
      "outputs": [],
      "source": [
        "for (image, mask_seg1, mask_seg2, mask_center) in train_loader:\n",
        "  image = image.to(device)\n",
        "  output_top, output_bot = net_custom_loss_chase(image)\n",
        "\n",
        "\n",
        "  mask_top = torch.squeeze(output_top.cpu().detach()).numpy()\n",
        "  mask_bot = torch.squeeze(output_bot.cpu().detach()).numpy()\n",
        "\n",
        "  mask_seg1 = torch.squeeze(mask_seg1.cpu().detach()).numpy()\n",
        "  mask_center = torch.squeeze(mask_center.cpu().detach()).numpy()\n",
        "\n",
        "  print(\"--------------------------------------------\")\n",
        "\n",
        "  plt.hist(mask_top.flatten())\n",
        "  plt.title(\"Histogram of segmentation mask\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.hist(mask_top.flatten())\n",
        "  plt.title(\"Histogram of centerline extraction mask\")\n",
        "  plt.show()    \n",
        "\n",
        "\n",
        "  print(\"Metrics for segmentation: \")\n",
        "  print(\"SE = \",SE(mask_top, mask_seg1))\n",
        "  print(\"SP = \",SP(mask_top, mask_seg1))\n",
        "  print(\"accuracy = \",accuracy(mask_top, mask_seg1))\n",
        "  print(\"AUC = \", AUC(mask_top, mask_seg1))\n",
        "\n",
        "\n",
        "  print(\"Metrics for centerline extraction: \")\n",
        "  print(\"SE = \",SE(mask_bot, mask_center))\n",
        "  print(\"SP = \",SP(mask_bot, mask_center))\n",
        "  print(\"accuracy = \",accuracy(mask_bot, mask_center))\n",
        "  print(\"AUC = \", AUC(mask_bot, mask_center))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(16,9))\n",
        "  plt.imshow(torch.squeeze(output_top.cpu().detach()).numpy(), cmap=\"gray\")\n",
        "  plt.title(\"Segmentation mask (logits)\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(16,9))\n",
        "  plt.imshow(torch.squeeze(output_bot.cpu().detach()).numpy(), cmap=\"gray\")\n",
        "  plt.title(\"Centerline extraction mask (logits)\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkGIGH-MLkmc"
      },
      "source": [
        "### CHASE BCE loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM1x1BJjB_51"
      },
      "outputs": [],
      "source": [
        "n_epoch = 100\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "net_BCE_loss_chase = MSCNet(n_channels = 1, n_classes = 1, inner_sizes=(59, 114, 234, 474), output_size=960)\n",
        "net_BCE_loss_chase.to(device)\n",
        "\n",
        "dataset = DataLoaderCHASE(\"data/chase\")\n",
        "train, test = torch.utils.data.random_split(dataset, [21,7], generator = torch.Generator().manual_seed(123))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=1, num_workers=0, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1, num_workers=0, pin_memory=True, drop_last=True)\n",
        "\n",
        "optimizer = optim.Adam(net_BCE_loss_chase.parameters(), lr = 1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.9)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  running_loss = 0.0\n",
        "  net_BCE_loss_chase.train()\n",
        "\n",
        "  for (image, mask_seg1, mask_seg2, mask_center) in tqdm(train_loader):\n",
        "    image = image.to(device)\n",
        "    mask_seg1 = mask_seg1.to(device)\n",
        "    mask_seg2 = mask_seg2.to(device)\n",
        "    mask_center = mask_center.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output_top, output_bot = net_BCE_loss_chase(image)\n",
        "\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    loss = criterion(torch.squeeze(output_top), torch.squeeze(mask_seg1)) + criterion(torch.squeeze(output_bot), torch.squeeze(mask_seg2)) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  train_loss=running_loss/len(train_loader)\n",
        "  train_losses.append(train_loss)\n",
        "  \n",
        "def test(epoch):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  net_BCE_loss_chase.eval()  \n",
        "\n",
        "  with torch.no_grad():\n",
        "    for (image, mask_seg1, mask_seg2, mask_center) in tqdm(test_loader):\n",
        "      image = image.to(device)\n",
        "      mask_seg1 = mask_seg1.to(device)\n",
        "      mask_seg2 = mask_seg2.to(device)\n",
        "      mask_center = mask_center.to(device)\n",
        "      \n",
        "      output_top, output_bot = net_BCE_loss_chase(image)\n",
        "\n",
        "      criterion = nn.BCEWithLogitsLoss()\n",
        "      loss = criterion(torch.squeeze(output_top), torch.squeeze(mask_seg1)) + criterion(torch.squeeze(output_bot), torch.squeeze(mask_seg2)) \n",
        "\n",
        "\n",
        "      running_loss+=loss.item()\n",
        "      \n",
        "  \n",
        "  test_loss=running_loss/len(test_loader)\n",
        "\n",
        "  eval_losses.append(test_loss)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(n_epoch+1):\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  scheduler.step(eval_losses[-1])\n",
        "  print(f\"Epoch {epoch+1}, training_loss = {train_losses[-1]:.6f}, val_loss = {eval_losses[-1]:.6f}\")\n",
        "  if epoch % 50 == 0:\n",
        "    torch.save(net_BCE_loss_chase.state_dict(), f'BCE_loss_chase_{epoch}.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FcSe6FMLzd9"
      },
      "source": [
        "### Results preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpnM2PSwA7xX"
      },
      "outputs": [],
      "source": [
        "for (image, mask_seg1, mask_seg2, mask_center) in train_loader:\n",
        "  image = image.to(device)\n",
        "  output_top, output_bot = net_BCE_loss_chase(image)\n",
        "\n",
        "\n",
        "  mask_top = torch.squeeze(output_top.cpu().detach()).numpy()\n",
        "  mask_bot = torch.squeeze(output_bot.cpu().detach()).numpy()\n",
        "\n",
        "  mask_seg1 = torch.squeeze(mask_seg1.cpu().detach()).numpy()\n",
        "  mask_seg2 = torch.squeeze(mask_seg2.cpu().detach()).numpy()\n",
        "\n",
        "  print(\"--------------------------------------------\")\n",
        "\n",
        "  plt.hist(mask_top.flatten())\n",
        "  plt.title(\"Histogram of segmentation mask\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.hist(mask_top.flatten())\n",
        "  plt.title(\"Histogram of negative segmentation mask\")\n",
        "  plt.show()    \n",
        "\n",
        "\n",
        "  print(\"Metrics for segmentation: \")\n",
        "  print(\"SE = \",SE(mask_top, mask_seg1))\n",
        "  print(\"SP = \",SP(mask_top, mask_seg1))\n",
        "  print(\"accuracy = \",accuracy(mask_top, mask_seg1))\n",
        "  print(\"AUC = \", AUC(mask_top, mask_seg1))\n",
        "\n",
        "\n",
        "  print(\"Metrics for negative segmentation: \")\n",
        "  print(\"SE = \",SE(mask_bot, mask_seg2))\n",
        "  print(\"SP = \",SP(mask_bot, mask_seg2))\n",
        "  print(\"accuracy = \",accuracy(mask_bot, mask_seg2))\n",
        "  print(\"AUC = \", AUC(mask_bot, mask_seg2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(16,9))\n",
        "  plt.imshow(torch.squeeze(output_top.cpu().detach()).numpy(), cmap=\"gray\")\n",
        "  plt.title(\"Segmentation mask (logits)\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(16,9))\n",
        "  plt.imshow(torch.squeeze(output_bot.cpu().detach()).numpy(), cmap=\"gray\")\n",
        "  plt.title(\"Negative segmentation mask (logits)\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt5YxYVsL85Y"
      },
      "source": [
        "### DRIVE custom loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzLyqsqVugQW",
        "outputId": "15746c00-f719-4ed6-bb64-9eba9b176eab"
      },
      "outputs": [],
      "source": [
        "n_epoch = 100\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "net_custom_loss_drive = MSCNet(n_channels = 1, n_classes = 1, inner_sizes=(35, 66, 138, 282), output_size=576)\n",
        "net_custom_loss_drive.to(device)\n",
        "\n",
        "dataset = DataLoaderDRIVE(\"data/drive\")\n",
        "train, test = torch.utils.data.random_split(dataset, [15,5], generator = torch.Generator().manual_seed(123))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=1, num_workers=0, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1, num_workers=0, pin_memory=True, drop_last=True)\n",
        "\n",
        "optimizer = optim.Adam(net_custom_loss_drive.parameters(), lr = 1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.9)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  running_loss = 0.0\n",
        "  net_custom_loss_drive.train()\n",
        "\n",
        "  for (image, mask_seg1, mask_seg2, mask_center) in tqdm(train_loader):\n",
        "    image = image.to(device)\n",
        "    mask_seg1 = mask_seg1.to(device)\n",
        "    mask_seg2 = mask_seg2.to(device)\n",
        "    mask_center = mask_center.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output_top, output_bot = net_custom_loss_drive(image)\n",
        "\n",
        "\n",
        "    criterion = CustomLoss(0.2, 0.5, 0.1, threshold=0.63)\n",
        "    loss = criterion(torch.squeeze(output_top), torch.squeeze(mask_seg1), torch.squeeze(output_bot), torch.squeeze(mask_center))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  train_loss=running_loss/len(train_loader)\n",
        "  train_losses.append(train_loss)\n",
        "  \n",
        "def test(epoch):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  net_custom_loss_drive.eval()  \n",
        "\n",
        "  with torch.no_grad():\n",
        "    for (image, mask_seg1, mask_seg2, mask_center) in tqdm(test_loader):\n",
        "      image = image.to(device)\n",
        "      mask_seg1 = mask_seg1.to(device)\n",
        "      mask_seg2 = mask_seg2.to(device)\n",
        "      mask_center = mask_center.to(device)\n",
        "      \n",
        "      output_top, output_bot = net_custom_loss_drive(image)\n",
        "\n",
        "      criterion = CustomLoss(0.2, 0.5, 0.1, threshold=0.63)\n",
        "      loss = criterion(torch.squeeze(output_top), torch.squeeze(mask_seg1), torch.squeeze(output_bot), torch.squeeze(mask_center))\n",
        "\n",
        "      running_loss+=loss.item()\n",
        "      \n",
        "  \n",
        "  test_loss=running_loss/len(test_loader)\n",
        "\n",
        "  eval_losses.append(test_loss)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(n_epoch+1):\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  scheduler.step(eval_losses[-1])\n",
        "  print(f\"Epoch {epoch+1}, training_loss = {train_losses[-1]:.6f}, val_loss = {eval_losses[-1]:.6f}\")\n",
        "  if epoch % 50 == 0:\n",
        "    torch.save(net_custom_loss_drive.state_dict(), f'custom_loss_drive_{epoch}.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87RtF7lyMAEa"
      },
      "source": [
        "### Results preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LN9bjksSvQT6",
        "outputId": "92627b4a-e3ad-4e19-dc45-d10d2191d99d"
      },
      "outputs": [],
      "source": [
        "ses = []\n",
        "sps = []\n",
        "accs = []\n",
        "aucs = []\n",
        "\n",
        "net_custom_loss_drive = MSCNet(n_channels = 1, n_classes = 1, inner_sizes=(35, 66, 138, 282), output_size=576)\n",
        "net_custom_loss_drive.to(device)\n",
        "\n",
        "dataset = DataLoaderDRIVE(\"data/drive\")\n",
        "train, test = torch.utils.data.random_split(dataset, [15,5], generator = torch.Generator().manual_seed(123))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=1, num_workers=0, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1, num_workers=0, pin_memory=True, drop_last=True)\n",
        "\n",
        "net_custom_loss_drive.load_state_dict(torch.load(\"drive/MyDrive/wb/custom_loss_drive_100.pth\"))\n",
        "\n",
        "\n",
        "for (image, mask_seg1, mask_seg2, mask_center) in test_loader:\n",
        "  image = image.to(device)\n",
        "  output_top, output_bot = net_custom_loss_drive(image)\n",
        "\n",
        "\n",
        "  mask_top = torch.squeeze(output_top.cpu().detach()).numpy()\n",
        "  mask_bot = torch.squeeze(output_bot.cpu().detach()).numpy()\n",
        "\n",
        "  mask_seg1 = torch.squeeze(mask_seg1.cpu().detach()).numpy()\n",
        "  mask_center = torch.squeeze(mask_center.cpu().detach()).numpy()\n",
        "\n",
        "  print(\"--------------------------------------------\")\n",
        "\n",
        "  plt.hist(mask_top.flatten())\n",
        "  plt.title(\"Histogram of segmentation mask\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.hist(mask_bot.flatten())\n",
        "  plt.title(\"Histogram of centerline extraction mask\")\n",
        "  plt.show()    \n",
        "\n",
        "\n",
        "  ses.append(SE(mask_top, mask_seg1))\n",
        "  sps.append(SP(mask_top, mask_seg1))\n",
        "  accs.append(accuracy(mask_top, mask_seg1))\n",
        "  aucs.append(AUC(mask_top, mask_seg1))\n",
        "\n",
        "  print(\"Metrics for segmentation: \")\n",
        "  print(\"SE = \",ses[-1])\n",
        "  print(\"SP = \",sps[-1])\n",
        "  print(\"accuracy = \",accs[-1])\n",
        "  print(\"AUC = \", aucs[-1])\n",
        "\n",
        "\n",
        "  print(\"Metrics for centerline extraction: \")\n",
        "  print(\"SE = \",SE(mask_bot, mask_center))\n",
        "  print(\"SP = \",SP(mask_bot, mask_center))\n",
        "  print(\"accuracy = \",accuracy(mask_bot, mask_center))\n",
        "  print(\"AUC = \", AUC(mask_bot, mask_center))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(16,9))\n",
        "  plt.imshow(torch.squeeze(output_top.cpu().detach()).numpy(), cmap=\"gray\")\n",
        "  plt.title(\"Segmentation mask (logits)\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(16,9))\n",
        "  plt.imshow(torch.squeeze(output_bot.cpu().detach()).numpy(), cmap=\"gray\")\n",
        "  plt.title(\"Centerline extraction mask (logits)\")\n",
        "  plt.show()\n",
        "\n",
        "print(\"DRIVE, custom loss SE = {:.4f}\".format(sum(ses)/len(ses)))\n",
        "print(\"DRIVE, custom loss SP = {:.4f}\".format(sum(sps)/len(sps)))\n",
        "print(\"DRIVE, custom loss ACC = {:.4f}\".format(sum(accs)/len(accs)))\n",
        "print(\"DRIVE, custom loss AUC = {:.4f}\".format(sum(aucs)/len(aucs)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4zaJcOIMG4G"
      },
      "source": [
        "### DRIVE BCE loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyBfliOvvQ8R",
        "outputId": "681c8e02-b555-404d-cd71-d5c9297b0d2e"
      },
      "outputs": [],
      "source": [
        "n_epoch = 100\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "net_BCE_loss_drive = MSCNet(n_channels = 1, n_classes = 1, inner_sizes=(35, 66, 138, 282), output_size=576)\n",
        "net_BCE_loss_drive.to(device)\n",
        "\n",
        "dataset = DataLoaderDRIVE(\"data/drive\")\n",
        "train, test = torch.utils.data.random_split(dataset, [15,5], generator = torch.Generator().manual_seed(123))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=1, num_workers=0, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1, num_workers=0, pin_memory=True, drop_last=True)\n",
        "\n",
        "optimizer = optim.Adam(net_BCE_loss_drive.parameters(), lr = 1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.9)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  running_loss = 0.0\n",
        "  net_BCE_loss_drive.train()\n",
        "\n",
        "  for (image, mask_seg1, mask_seg2, mask_center) in tqdm(train_loader):\n",
        "    image = image.to(device)\n",
        "    mask_seg1 = mask_seg1.to(device)\n",
        "    mask_seg2 = mask_seg2.to(device)\n",
        "    mask_center = mask_center.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output_top, output_bot = net_BCE_loss_drive(image)\n",
        "\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    loss = criterion(torch.squeeze(output_top), torch.squeeze(mask_seg1)) + criterion(torch.squeeze(output_bot), torch.squeeze(mask_seg2)) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  train_loss=running_loss/len(train_loader)\n",
        "  train_losses.append(train_loss)\n",
        "  \n",
        "def test(epoch):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  net_BCE_loss_drive.eval()  \n",
        "\n",
        "  with torch.no_grad():\n",
        "    for (image, mask_seg1, mask_seg2, mask_center) in tqdm(test_loader):\n",
        "      image = image.to(device)\n",
        "      mask_seg1 = mask_seg1.to(device)\n",
        "      mask_seg2 = mask_seg2.to(device)\n",
        "      mask_center = mask_center.to(device)\n",
        "      \n",
        "      output_top, output_bot = net_BCE_loss_drive(image)\n",
        "\n",
        "      criterion = nn.BCEWithLogitsLoss()\n",
        "      loss = criterion(torch.squeeze(output_top), torch.squeeze(mask_seg1)) + criterion(torch.squeeze(output_bot), torch.squeeze(mask_seg2)) \n",
        "\n",
        "\n",
        "      running_loss+=loss.item()\n",
        "      \n",
        "  \n",
        "  test_loss=running_loss/len(test_loader)\n",
        "\n",
        "  eval_losses.append(test_loss)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(n_epoch+1):\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  scheduler.step(eval_losses[-1])\n",
        "  print(f\"Epoch {epoch+1}, training_loss = {train_losses[-1]:.6f}, val_loss = {eval_losses[-1]:.6f}\")\n",
        "  if epoch % 50 == 0:\n",
        "    torch.save(net_BCE_loss_drive.state_dict(), f'BCE_loss_drive_{epoch}.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF83P8caMLbP"
      },
      "source": [
        "### Results preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a1tsuQOkzY9o",
        "outputId": "bb949821-3e3b-4809-e031-9239a6a6ebef"
      },
      "outputs": [],
      "source": [
        "ses = []\n",
        "sps = []\n",
        "accs = []\n",
        "aucs = []\n",
        "\n",
        "net_BCE_loss_drive = MSCNet(n_channels = 1, n_classes = 1, inner_sizes=(35, 66, 138, 282), output_size=576)\n",
        "net_BCE_loss_drive.to(device)\n",
        "\n",
        "dataset = DataLoaderDRIVE(\"data/drive\")\n",
        "train, test = torch.utils.data.random_split(dataset, [15,5], generator = torch.Generator().manual_seed(123))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=1, num_workers=0, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, shuffle=False, batch_size=1, num_workers=0, pin_memory=True, drop_last=True)\n",
        "\n",
        "net_BCE_loss_drive.load_state_dict(torch.load(\"drive/MyDrive/wb/BCE_loss_drive_100.pth\"))\n",
        "\n",
        "\n",
        "for (image, mask_seg1, mask_seg2, mask_center) in test_loader:\n",
        "  image = image.to(device)\n",
        "  output_top, output_bot = net_BCE_loss_drive(image)\n",
        "\n",
        "\n",
        "  mask_top = torch.squeeze(output_top.cpu().detach()).numpy()\n",
        "  mask_bot = torch.squeeze(output_bot.cpu().detach()).numpy()\n",
        "\n",
        "  mask_seg1 = torch.squeeze(mask_seg1.cpu().detach()).numpy()\n",
        "  mask_seg2 = torch.squeeze(mask_seg2.cpu().detach()).numpy()\n",
        "\n",
        "  print(\"--------------------------------------------\")\n",
        "\n",
        "  plt.hist(mask_top.flatten())\n",
        "  plt.title(\"Histogram of segmentation mask\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.hist(mask_top.flatten())\n",
        "  plt.title(\"Histogram of negative segmentation mask\")\n",
        "  plt.show()    \n",
        "\n",
        "\n",
        "  ses.append(SE(mask_top, mask_seg1))\n",
        "  sps.append(SP(mask_top, mask_seg1))\n",
        "  accs.append(accuracy(mask_top, mask_seg1))\n",
        "  aucs.append(AUC(mask_top, mask_seg1))\n",
        "\n",
        "  print(\"Metrics for segmentation: \")\n",
        "  print(\"SE = \",ses[-1])\n",
        "  print(\"SP = \",sps[-1])\n",
        "  print(\"accuracy = \",accs[-1])\n",
        "  print(\"AUC = \", aucs[-1])\n",
        "\n",
        "\n",
        "\n",
        "  print(\"Metrics for negative segmentation: \")\n",
        "  print(\"SE = \",SE(mask_bot, mask_seg2))\n",
        "  print(\"SP = \",SP(mask_bot, mask_seg2))\n",
        "  print(\"accuracy = \",accuracy(mask_bot, mask_seg2))\n",
        "  print(\"AUC = \", AUC(mask_bot, mask_seg2))\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(16,9))\n",
        "  plt.imshow(mask_top, cmap=\"gray\")\n",
        "  plt.title(\"Segmentation mask (logits)\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(16,9))\n",
        "  plt.imshow(mask_bot, cmap=\"gray\")\n",
        "  plt.title(\"Negative segmentation mask (logits)\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "print(\"DRIVE, custom loss SE = {:.4f}\".format(sum(ses)/len(ses)))\n",
        "print(\"DRIVE, custom loss SP = {:.4f}\".format(sum(sps)/len(sps)))\n",
        "print(\"DRIVE, custom loss ACC = {:.4f}\".format(sum(accs)/len(accs)))\n",
        "print(\"DRIVE, custom loss AUC = {:.4f}\".format(sum(aucs)/len(aucs)))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "wb_checkpoint2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "5c78c83bff0417342952dc0c0896a7258fe24f94fa556fee9b04f2330eb27859"
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit (system)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
